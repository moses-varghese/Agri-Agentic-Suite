version: '3.8'

services:
  db:
    image: postgres:15-alpine
    container_name: agri_db
    # We now use a named volume called 'db_data' instead of a host path.
    volumes:
      - db_data:/var/lib/postgresql/data
    env_file:
      - ./.env
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "5432:5432"
    networks:
      - agri_net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
  
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/init-ollama.sh:/init-ollama.sh
    ports:
      - "11434:11434"
    networks:
      - agri_net
    entrypoint: /init-ollama.sh # <-- Use entrypoint instead of command
    command: ["${LOCAL_LLM_MODEL}", "${VISION_MODEL}"]
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q '${LOCAL_LLM_MODEL}' && ollama list | grep -q '${VISION_MODEL}'"]
      # test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 10s
      timeout: 30s
      retries: 10
      start_period: 560s

  groundtruth_ai:
    container_name: groundtruth_ai_service
    build:
      context: .
      dockerfile: ./services/groundtruth_ai/Dockerfile
    volumes:
      - ./local_data:/app/local_data
      - chroma_db:/chroma_db 
      # this is a named volume which should be on and not the bind volume but bind volume can be on to just inspect. 
      # two same volume of bind and named cannot be activated. if both on, then the last one under volumes mentioned would be active. named vol managed 
      # by docker and cannot be accessed by user, bind volume managed and accessed by user 
      - ./knowledge_documents:/app/knowledge_documents
      # - ./chroma_db_volume:/chroma_db 
      # this is for inspecting vectordb by user to check update progress which is a bind volume
    env_file:
      - ./.env
    ports:
      - "8001:8000"
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - agri_net
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000

  yieldwise:
    container_name: yieldwise_service
    build:
      context: .
      dockerfile: ./services/yieldwise/Dockerfile
    volumes:
      - ./local_data:/app/local_data
      - ./knowledge_documents:/app/knowledge_documents
      - chroma_db:/chroma_db 
    env_file:
      - ./.env
    ports:
      - "8002:8000"
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - agri_net
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000

  fieldscout_ai:
    container_name: fieldscout_ai_service
    build:
      context: .
      dockerfile: ./services/fieldscout_ai/Dockerfile
    env_file:
      - ./.env
    ports:
      - "8003:8000"
    depends_on:
      db:
        condition: service_healthy
    networks:
      - agri_net
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000

  frontend:
    container_name: frontend_service
    build:
      context: ./frontend
      args:
        - NEXT_PUBLIC_GROUNDTRUTH_AI_URL=http://localhost:8001/generate-truth
        - NEXT_PUBLIC_GROUNDTRUTH_AI_VOICE_URL=http://localhost:8001/voice-query
        - NEXT_PUBLIC_YIELDWISE_URL=http://localhost:8002/generate-plan
        - NEXT_PUBLIC_FIELDSCOUT_AI_URL=http://localhost:8003/diagnose
    ports:
      - "3000:3000"
    networks:
      - agri_net
    depends_on:
      - groundtruth_ai
      - yieldwise
      - fieldscout_ai

networks:
  agri_net: 
    driver: bridge

# We define the named volume here at the top level.
volumes:
  db_data:
  ollama_data:
  chroma_db: {}